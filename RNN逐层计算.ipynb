{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b82adca-ebfa-484d-bda5-c91781eed398",
   "metadata": {},
   "source": [
    "<h2>循环神经网络</h2>\n",
    "<p>基础公式：</p>\n",
    "<p>$$\\mathbf{h_{t}} = f(\\mathbf{U}\\mathbf{h_{t-1}}+\\mathbf{W}\\mathbf{x_{t}}+\\mathbf{b})$$</p>\n",
    "<p>其中\\(f(*) \\)是非线性激活函数</p>\n",
    "<p><img src=\"https://pic1.zhimg.com/70/v2-397376bafce3f0c634f9f73161044c52_1440w.image?source=172ae18b&amp;biz_tag=Post\" alt=\"Pytorch实现RNN\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6ddd979-8897-42cc-9e68-d9acc6206359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0: tensor([[-0.0075]])\n",
      "weight_hh_l0: tensor([[0.5364]])\n",
      "bias_ih_l0: tensor([-0.8230])\n",
      "bias_hh_l0: tensor([-0.7359])\n",
      "hn:tensor([[[0.]]], grad_fn=<StackBackward0>)\n",
      "output:tensor([[[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# 创建一维数据和label，用pytorch创建一个最简单的RNN网络\n",
    "# 确保数据和标签的数据类型为 torch.float（即 torch.float32） RNN期待的输入数据通常是三维的，其形状为 (序列长度, 批量大小, 特征数量)。\n",
    "data = torch.tensor([1,2,3], dtype=torch.float).reshape(3, 1,1)\n",
    "label = torch.tensor([2., 3., 4.], dtype=torch.float)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 输入为1，中间隐藏层数量也为1, 堆叠层数为1 第一个参数是输入的特征数量，第二个参数是隐藏层的特征数量。你的数据形状需要与这些参数相匹配\n",
    "# 隐藏层的特征数量决定了网络的输出形状，在这个例子中，输出为简单的一维数字，因此隐藏层的形状为1\n",
    "rnn = nn.RNN(1, 1, num_layers=1, nonlinearity='relu')\n",
    "# 初始化隐藏状态 层数，批量大小，隐藏层形状\n",
    "# h0 = torch.zeros(3, 1, 1)\n",
    "\n",
    "output, hn = rnn(data)\n",
    "\n",
    "# 输出参数\n",
    "for name, param in rnn.named_parameters():\n",
    "    # print(f\"{name}: {param.size()}\")\n",
    "    print(f\"{name}: {param.data}\" )\n",
    "print(f\"hn:{hn}\")\n",
    "print(f\"output:{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a543f-9159-418b-8832-dcb0638e8f36",
   "metadata": {},
   "source": [
    "<p>前向计算：</p>\n",
    "<p>$$ h_{0}=f(-0.0075*1+0.5364*0+(-0.8230)+(-0.7356))=f(-1.56)=0 $$</p>\n",
    "<p> 参数共享：</p>\n",
    "<p>即使你有一个序列长度为3的many-to-many RNN，这个RNN在每一个时间步上都会使用相同的权重和偏置来处理输入和前一个时间步的隐藏状态。这就是所谓的参数共享，它是RNN能够处理任意长度序列的关键特性。因此，不管序列长度是多少，隐藏层的参数数量都是一样的 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203585b5-1b85-4cdc-9669-260be0e4d80e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
